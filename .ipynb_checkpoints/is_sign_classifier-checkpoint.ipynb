{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a32ab3d",
   "metadata": {
    "id": "5a32ab3d"
   },
   "source": [
    "This is a Neural-Network, that determines if the image (data) is a sign, or not. (2 labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7de4692a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "executionInfo": {
     "elapsed": 11997,
     "status": "error",
     "timestamp": 1768423910987,
     "user": {
      "displayName": "Göblyös Bence",
      "userId": "08664305870357546122"
     },
     "user_tz": -60
    },
    "id": "7de4692a",
    "outputId": "afd61e52-db80-4c49-fd45-7cff60c7967a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images...\n",
      "  Finished no: Loaded 83 images.\n",
      "  Finished yes: Loaded 288 images.\n",
      "\n",
      "Total Dataset: 371 images loaded.\n",
      "Feature vector size: 3072 (32x32x3 pixels)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training..\n",
      "\n",
      "--- Evaluation ---\n",
      "Model Accuracy: 97.33%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       1.00      0.91      0.95        23\n",
      "         yes       0.96      1.00      0.98        52\n",
      "\n",
      "    accuracy                           0.97        75\n",
      "   macro avg       0.98      0.96      0.97        75\n",
      "weighted avg       0.97      0.97      0.97        75\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Dictionary - arrow types and labels\n",
    "folders = {\n",
    "    \"no\": 0,\n",
    "    \"yes\": 1\n",
    "}\n",
    "\n",
    "# The parent folder in drive\n",
    "base_folder = \"data/is_sign/train\"\n",
    "\n",
    "\n",
    "# resize img\n",
    "IMG_SIZE = (32, 32)\n",
    "\n",
    "\n",
    "X_data = [] #  pixel values\n",
    "y_data = [] #  labels\n",
    "\n",
    "print(\"Loading images...\")\n",
    "\n",
    "for folder_name, label in folders.items():\n",
    "\n",
    "    # File path in drive\n",
    "    folder_path = os.path.join(base_folder, folder_name)\n",
    "\n",
    "    # IF NO folder, SKIP\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Warning: Folder not found: {folder_path}\")\n",
    "        continue\n",
    "\n",
    "    # Get a list of ALL files in this folder\n",
    "    files = os.listdir(folder_path)\n",
    "\n",
    "    count = 0\n",
    "    for filename in files:\n",
    "        # if PNG file\n",
    "        if filename.lower().endswith(\".png\"):\n",
    "\n",
    "            #  full path\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "            try:\n",
    "                img = cv2.imread(file_path)\n",
    "\n",
    "                if img is not None:\n",
    "                    # Resize, Flatten, Normalize\n",
    "                    img = cv2.resize(img, IMG_SIZE)\n",
    "                    flat_img = img.flatten()\n",
    "                    norm_img = flat_img / 255.0\n",
    "\n",
    "                    X_data.append(norm_img)\n",
    "                    y_data.append(label)\n",
    "                    count += 1\n",
    "                else:\n",
    "                    print(f\"Warning: Could not open {filename}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {filename}: {e}\")\n",
    "\n",
    "    print(f\"  Finished {folder_name}: Loaded {count} images.\")\n",
    "\n",
    "# Convert lists to np\n",
    "X = np.array(X_data)\n",
    "y = np.array(y_data)\n",
    "\n",
    "\n",
    "if len(X) == 0:\n",
    "    print(\"Error: No images found. Check folder structure and paths.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"\\nTotal Dataset: {len(X)} images loaded.\")\n",
    "print(f\"Feature vector size: {X.shape[1]} (32x32x3 pixels)\")\n",
    "\n",
    "#plot an img\n",
    "image_index = 0\n",
    "flat_image = X[image_index]  #  1D array of flattened image\n",
    "\n",
    "# Reshape it back to (Height, Width, Channels)\n",
    "# use IMG_SIZE (32, 32) and 3 channels\n",
    "restored_image = flat_image.reshape(32, 32, 3)\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(restored_image)\n",
    "plt.title(f\"Label: {y[image_index]}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"\\nTraining..\")\n",
    "\n",
    "#  Multi-Layer Perceptron Classifier\n",
    "# hidden_layer_sizes=(100, 50): Two layers of \"neurons\"\n",
    "\n",
    "clf = MLPClassifier(\n",
    "    hidden_layer_sizes=(100, 50),\n",
    "    max_iter=500,\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n--- Evaluation ---\")\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "acc = accuracy_score(y_test, predictions)\n",
    "print(f\"Model Accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "# detailed report\n",
    "target_names = list(folders.keys())\n",
    "print(classification_report(y_test, predictions, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "t-0jBKj4Rvs6",
   "metadata": {
    "id": "t-0jBKj4Rvs6"
   },
   "outputs": [],
   "source": [
    "M1, M2, M3 = clf.coefs_\n",
    "model_dir = \"models/is_sign\"\n",
    "np.savetxt(f\"{model_dir}/M1.csv\", M1, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "95bc8794-fde2-4679-b7e1-8471b444e942",
   "metadata": {},
   "outputs": [],
   "source": [
    "b1, b2, b3 = clf.intercepts_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "933af2d3-bbaf-49f8-9773-4263a67c18a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f3831002-0080-4325-816b-7104a535d27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fa89356d-a079-4edd-94f3-4760ab7602f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.23702363e-02,  1.34412224e-07, -4.35701884e-02, ...,\n",
       "        -1.03873203e-02, -4.73786313e-02, -6.15675072e-08],\n",
       "       [-2.02291145e-02,  3.71658678e-10,  3.34719134e-02, ...,\n",
       "        -1.05967358e-02,  6.01712473e-03,  2.97009719e-05],\n",
       "       [ 3.39909891e-02, -7.36152159e-09,  3.60518690e-02, ...,\n",
       "        -1.11516283e-02,  3.78540045e-02, -3.63952526e-06],\n",
       "       ...,\n",
       "       [ 2.12567369e-02, -1.48938279e-05, -8.83551624e-03, ...,\n",
       "        -1.21751188e-02, -4.13417140e-02, -6.89590296e-08],\n",
       "       [ 3.27733701e-02, -4.76971274e-10,  3.61547814e-02, ...,\n",
       "        -1.05078439e-02,  2.41677471e-02,  7.88533830e-08],\n",
       "       [-1.84150772e-02,  1.75042918e-06,  3.34791500e-02, ...,\n",
       "        -1.14724247e-02,  2.33490874e-02,  8.39530630e-08]],\n",
       "      shape=(3072, 100))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.loadtxt(f\"{model_dir}/M1.csv\", delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f090a37f-b5eb-47d3-8708-ff56884b80ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def homebrewMLP(X):\n",
    "    l1 = relu(np.matmul(X, M1) + b1)\n",
    "    l2 = relu(np.matmul(l1, M2) + b2)\n",
    "    l3 = np.matmul(l2, M3) + b3\n",
    "    return 1 if l3[0] > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7bfea360-f78e-4366-8db6-f9c6eaf2eca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3072,)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19785145-20d2-4c0b-a103-2db3181f6e41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
